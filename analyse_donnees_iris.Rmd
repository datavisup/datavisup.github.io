---
title: "Introduction à l'analyse des données avec iris"
author: "Kezhan SHI"
date: "16 Novembre 2015"
output:
    html_document:
      toc: true
      toc_float: true
      number_sections: true
      theme: journal
      highlight: textmate
      code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning = FALSE)
```

# Introduction

Une collection de fonctions simples pour analyser une série de données.

# Analyse statistique simple 

## Résumé des données

```{r}
summary(iris)
```

```{r}
plot(iris)
```

## Analyse univariée

Comprendre le comportement d'une variable

- `quantile` pour afficher les quantiles, et on peut utiliser l'option `probs` pour définir les probabilités qu'on souhaite afficher
- `mean` pour la moyenne, `var` pour la variance et `sd` pour l'écart-type
- `hist` pour afficher un histogramme
- `plot(density())` pour voir la densité
- Pour aller plus loin : utiliser `ggplot` pour contrôler la fenêtre du noyau, superposer plusieurs distributions

```{r}
quantile(iris$Sepal.Length)
```

## Analyse multivariée

Comprendre la relation entre 2 ou plus de variables

- Calculer les coefficients de corrélation linéaire
- Calculer la matrice de covariance et la matrice de corrélation linéaire (`cov` et `cor`)
- Tracer des graphiques avec 2 variables représentées

# Visualisation des données à 2 variables

## Visualisation de 2 variables

```{r results='hide'}
plot(iris$Petal.Length, iris$Petal.Width, main="Edgar Anderson's Iris Data")
```

```{r}
plot(iris$Petal.Length, iris$Petal.Width, 
     pch=c(23,24,25)[unclass(iris$Species)], 
     bg=c("red","green3","blue")[unclass(iris$Species)], 
     main="Edgar Anderson's Iris Data")
```


## Visualisation 2 à 2

Améliorer le `plot(iris)`

- Mettre les couleurs
- Enlever la variable espèce
- Constater la symétrique du grahpique
- Afficher les corrélations linéaires
- Afficher les coefficients de corrélation linéaire sur la moitié du graphique

Fonctions R : `pairs()` ou `plot()`


Nuages de Points
-----
```{r}
pairs(iris[1:4], main = "Edgar Anderson's Iris Data", pch = 21, bg = c("red", "green3", "blue")[unclass(iris$Species)])
```

Calcul des coeff de corrélation linéaire
-----
```{r}
panel.pearson <- function(x, y, ...) {
  horizontal <- (par("usr")[1] + par("usr")[2]) / 2; 
  vertical <- (par("usr")[3] + par("usr")[4]) / 2; 
  text(horizontal, vertical, format(abs(cor(x,y)), digits=2)) 
}
```

Plot avec coeff de corrélation
------
```{r}
pairs(iris[1:4], main = "Edgar Anderson's Iris Data", pch = 21, bg = c("red","green3","blue")[unclass(iris$Species)], upper.panel=panel.pearson)
```


## D'autres visualisations par paire

```{r warning=FALSE}
library(GGally)
ggpairs(iris[1:4])
```

## Analyse multivariée en fonction des espèces

Comme on connaît les différentes espèces, on peut calculer les éléments statistiques en fonction des espèces 

- Tester `aggregate(,summary,data=iris)` pour différentes variables
- créer un `boxplot` pour les différentes variables

Boxplot
----
```{r warning=FALSE, message=FALSE}
boxplot(Sepal.Length~Species, data=iris)
```

## Tabplot

Distribution des variables en fonction des espèces

```{r warning=FALSE, message=FALSE}
# install.packages("tabplot")
library(tabplot)
tableplot(iris, sortCol="Species")
```


## Heatmap

```{r warning=FALSE, message=FALSE}
library(ggplot2)
distMatrix <- as.matrix(dist(iris[,1]))
heatmap(distMatrix)
```

## Parallel Cordinates

```{r}
library(MASS)
parcoord(iris[1:4], col=iris$Species)
```


# Visualisation de 3 variables

Jusqu'à maintenant, la visualisation est réalisée en dimension 2. Visualiser 3 variables dans l'espace 

## Visualisation en 3D

```{r warning=FALSE, message=FALSE}
library(scatterplot3d)
scatterplot3d(iris$Petal.Width, iris$Sepal.Length, iris$Sepal.Width)
```

## Visualisation de 3 variables interactive

```{r}
library(threejs)
scatterplot3js(iris$Petal.Width, iris$Sepal.Length, iris$Sepal.Width,color=c("black","steelblue","red")[unclass(iris$Species)])

```


## Visualisation de 3 variables en dim 2

```{r  warning=FALSE, message=FALSE}
library(ggplot2)
qplot(Sepal.Length,Petal.Length, data=iris,color=Species,size=Petal.Width)
```


```{r}
ggplot(iris, aes(Sepal.Length, Petal.Length, fill = Petal.Width)) + geom_tile()
```



```{r}
last_plot() + facet_wrap(~ Species)
```

# Techinques d'apprentissage automatique

## Exploration des données

Dans le graphique en 3D interactif, on voit qu'on peut tourner les points dans l'espace. Selon le point de vue, on distingue plus ou moins mieux les 3 classes.
L'idée d'analyse en composantes principales est de choisir un repère de façon à augmenter la variance inter classe.

lda permet de mieux distinguer les différentes classes.

## Classification non supervisée - k-means


```{r}
iris2 <- iris
iris2$Species <- NULL
(kmeans.result <- kmeans(iris2, 3))
```

### Classification non supervisée - k-means plot

```{r}
plot(iris2[c("Sepal.Length", "Sepal.Width")], col = kmeans.result$cluster)
points(kmeans.result$centers[,c("Sepal.Length", "Sepal.Width")], col = 1:3,pch = 8, cex=2)
```

### Classification non supervisée - k-means prédiction

```{r}
table(iris$Species, kmeans.result$cluster)
```


## Arbre de décision

Dans la base iris, on sait que trois espèces de fleurs sont représentées avec certaines caractéristiques (4 variables).
La question qu'on se pose, c'est si une nouvelle observation arrive (avec les valeurs des 4 mesures), comment peut-on savoir à quelle espace la fleur appartient ?

Rappel : processus global de la construction d'un algorithme.

### Arbre de décision - construction

- Créer deux bases : apprentisage et test
- Construire un arbre de décision sur la base d'apprentissage avec la librairie `party`, et la fonction `ctree(variable à expliquer ~ variable 1 + variable 2)`
- Afficher l'arbre avec `print` et `plot`

```{r message=FALSE, warning=FALSE}
ind <- sample(2, nrow(iris), replace=TRUE, prob=c(0.7, 0.3))
trainData <- iris[ind==1,]
testData <- iris[ind==2,]
library(party)
myFormula <- Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
iris_ctree <- ctree(myFormula, data=trainData)
```

### Arbre de décision - Affichage

```{r}
plot(iris_ctree)
print(iris_ctree)
```

### Arbre de décision - Exploitation

- Quelle est la varialbe la plus explicative ?
- Quelle est la précision de prédiction ?
- Combien a-t-on de branches et de feuilles ?

### Arbre de décision - table de confusion

```{r}
table(predict(iris_ctree), trainData$Species)
```

Calculer les taux de faux positif, faux négatifs, taux d'erreur


### Arbre de décision - Prédiction

```{r}
testPred <- predict(iris_ctree, newdata = testData)
table(testPred, testData$Species)
```

Calculer les taux de faux positif, faux négatifs, taux d'erreur
Pour aller plus loin : créer une boucle pour calculer un taux moyen d'erreur
Autre pacakge `rpart` pour construire un arbre de décision

## Random Forest - Introduction

Si on n'a pas fixé le `seed()` on voit que les échantillons d'apprentissage et de test sont choisis de façon aléatoire. 
La bagging permet de réaliser ces opérations automatiques. De plus, on peut aussi choisir les variables. Les deux randomisations ensemble, on obtient une forêt aléatoire.


```{r}
library(randomForest)
rf <- randomForest(Species ~ ., data=trainData, ntree=100, proximity=TRUE)
```


## Random Forest - Analyse

```{r}
table(predict(rf), trainData$Species)
print(rf)
attributes(rf)
```


```{r}
importance(rf)
varImpPlot(rf)
plot(rf)
```


### Random Forest - Prédiction 1

```{r}
irisPred <- predict(rf, newdata=testData)
table(irisPred, testData$Species)
```


```{r}
plot(margin(rf, testData$Species))
```
